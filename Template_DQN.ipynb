{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "> # Template for DQN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gym \n",
    "import copy\n",
    "import random \n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DQNModel(torch.nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden1 = 16, hidden2 = 126, hidden3 = 32):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(observation_space, hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2, hidden3)\n",
    "        self.fc4 = nn.Linear(hidden3, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def gen_epsilon_greedy_policy(estimator, n_action):\n",
    "    def policy_function(state, epsilon):\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, n_action - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = estimator(torch.from_numpy(state).float())\n",
    "            return torch.argmax(q_values).item()\n",
    "    return policy_function\n",
    "\n",
    "def train(optimizer, loss_fn, x, y):\n",
    "    loss = loss_fn(x, Variable(torch.Tensor(y)))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def main():\n",
    "\n",
    "    # Initialize the Environment\n",
    "    ENV_NAME = \"MountainCar-v0\"\n",
    "    env = gym.make(ENV_NAME)\n",
    "    # Define all the constants \n",
    "    EPOCHS = 500\n",
    "    GAMMA = 0.99\n",
    "    EPSILON = 1.0\n",
    "    EPSILON_DECAY = 0.98\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    observation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space.n \n",
    "\n",
    "    # Initialize the DL model and policy\n",
    "    DQN = DQNModel(observation_space, action_space)\n",
    "    policy = gen_epsilon_greedy_policy(DQN, action_space)\n",
    "    # initialize the optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(DQN.parameters(), LEARNING_RATE)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Some empty list to hold intermediatery values. \n",
    "    STEPS_LIST = list()\n",
    "    EPSILON_LIST = list()\n",
    "\n",
    "    for EPOCH in range(EPOCHS):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False \n",
    "        STEPS = 0\n",
    "\n",
    "        while (done == False):\n",
    "\n",
    "            action = policy(state, EPSILON)\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                newQ = DQN(torch.from_numpy(state_next).float())\n",
    "                maxQ = torch.max(newQ)\n",
    "\n",
    "            if reward == -1.0:\n",
    "                y = reward + GAMMA * maxQ\n",
    "            else:\n",
    "                y = reward\n",
    "\n",
    "            newQ[action] = y\n",
    "            q_values = DQN(torch.from_numpy(state).float())\n",
    "            train(optimizer, loss_fn, q_values, newQ)\n",
    "\n",
    "            STEPS += 1\n",
    "\n",
    "        STEPS_LIST.append(STEPS)\n",
    "        EPSILON_LIST.append(EPSILON)\n",
    "        print(\"Epoch: \" + str(EPOCH) + \", exploration: \" + str(EPSILON) + \", score: \" + str(STEPS))\n",
    "        EPSILON = max(EPSILON * EPSILON_DECAY, 0.01)\n",
    "\n",
    "    torch.save(DQN.state_dict(), './DQN_MountianCar.pth')\n",
    "\n",
    "    return STEPS_LIST, EPSILON_LIST"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\n",
    "    steps_list, epsilon_list = main()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(steps_list)\n",
    "plt.plot(epsilon_list)\n",
    "plt.title('Episode reward over time')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PyVenvRL': venv)"
  },
  "interpreter": {
   "hash": "f92183c92ecd334598bfdbb1c87fbf99fb0fe798b84a35b024353f555a04f511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}